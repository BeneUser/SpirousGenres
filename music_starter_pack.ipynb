{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa22828",
   "metadata": {},
   "source": [
    "### REI505M Final project: Music genre classification starter pack\n",
    "\n",
    "The following Dataset class operates on the GTZAN dataset.\n",
    "\n",
    "* The duration of most GTZAN files are 30 seconds (3022050=661500 samples) but some are slightly shorter (approx 29.9 seconds). For this reason we truncate at 660000 samples below.\n",
    "* It may be beneficial to work with smaller chunks than ~30 seconds.\n",
    "* You may want to perform the data augmentations in the `__get_item__` function.\n",
    "* For now, `train_dataset` contains all the dataset, you need to set aside some examples for validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d2051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.Conv1D import Conv1D\n",
    "from src.Config import Config\n",
    "from src.AudioDataset import AudioDataset\n",
    "from src.DataPreparation import get_partitioned_data\n",
    "from src.Augmentations import Augmentations\n",
    "import src.Utils as Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Training on:\", device)\n",
    "\n",
    "config = Config(#Path to folder with GTZAN files:\n",
    "                audio_dir_path='../music/',\n",
    "                fma_audio_dir_path='../fma_small/',\n",
    "                # music/\n",
    "                #  - rock/\n",
    "                #       rock.00099.wav\n",
    "                #       ...\n",
    "                #  - reggie/\n",
    "                #  ...\n",
    "                #  - blues/\n",
    "                #Choose how many genres we want to use:\n",
    "                num_genres=10, # eg. 2, 3, 5, 10\n",
    "                duration_size= 29, # länge der wav-files\n",
    "                sampling_rate = 22050,\n",
    "                #Data Partition\n",
    "                train_part_size=0.7,\n",
    "                val_part_size=0.15,\n",
    "                test_part_size=0.15,\n",
    "                batch_size=8, \n",
    "                learning_rate=1e-3,\n",
    "                epochs=50, \n",
    "                seed=42,\n",
    "                device=device)\n",
    "\n",
    "torch.manual_seed(config.seed) # Reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load num_genres from data and partition them\n",
    "train_files, train_labels, val_files, val_labels, test_files, test_labels = get_partitioned_data(config)\n",
    "\n",
    "#Setup Augmentations\n",
    "augmentations = Augmentations(#The augmentations, from which is chosen whenever augment() is called:\n",
    "                              available_augmentations= ['no_augment', 'no_augment', 'RandomStartCrop', 'FlipWave', 'ReverseWave'], #(names of functions in Augments-class)\n",
    "                              #available_augmentations= ['no_augment', 'no_augment', 'FlipWave', 'ReverseWave'], #(names of functions in Augments-class)\n",
    "                              num_augments=3, #Number of total augmentations (including those fixed in always_augment)\n",
    "                              always_augment=[0, 0, 1, 0, 0],   #Which augmentations to always use (bitmap on available_augmentations)\n",
    "                              cropt_size = 20,\n",
    "                              sampling_rate = config.sampling_rate)\n",
    "\n",
    "#Create Datasets and Dataloaders\n",
    "train_dataset = AudioDataset(audio_files=train_files, labels=train_labels, audio_path=config.audio_dir_path, \n",
    "                             sampling_rate=config.sampling_rate, \n",
    "                             duration=config.duration_size, #Duration *before* augmentation.\n",
    "                             augmentations=augmentations) #Only provide train_dataset with augmentations\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = AudioDataset(val_files, val_labels, config.audio_dir_path,\n",
    "                           sampling_rate= config.sampling_rate, duration= augmentations.cropt_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = AudioDataset(test_files, test_labels, config.audio_dir_path,\n",
    "                            sampling_rate=config.sampling_rate, duration=augmentations.cropt_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)                            \n",
    "\n",
    "print(\"Sanity Check Train Loader:\")\n",
    "tmp_features, tmp_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {tmp_features.size()}\")\n",
    "print(\"First sample: \", tmp_features[0])\n",
    "print(f\"Labels batch shape: {tmp_labels.size()}\")\n",
    "\n",
    "print(\"Sanity Check Valdidation Loader:\")\n",
    "tmp_features, tmp_labels = next(iter(val_loader))\n",
    "print(f\"Feature batch shape: {tmp_features.size()}\")\n",
    "print(\"First sample: \", tmp_features[0])\n",
    "print(f\"Labels batch shape: {tmp_labels.size()}\")\n",
    "\n",
    "print(\"Sanity Check Test Loader:\")\n",
    "tmp_features, tmp_labels = next(iter(test_loader))\n",
    "print(f\"Feature batch shape: {tmp_features.size()}\")\n",
    "print(\"First sample: \", tmp_features[0])\n",
    "print(f\"Labels batch shape: {tmp_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e8be490abbb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "n_classes = config.num_genres\n",
    "model = Conv1D(num_blocks=5,\n",
    "               num_conv_layers_per_block=1,\n",
    "               kernel_size=12,\n",
    "               num_first_layer_kernels=16,\n",
    "               conv_stride=4,\n",
    "               pool_stride=5,\n",
    "               dense_size=100,\n",
    "               do_batch_norm=True,\n",
    "               n_classes=n_classes,\n",
    "               config=config,\n",
    "               channels=[32, 64, 80, 96, 128],\n",
    "               kernel_sizes=[24, 12, 8, 6, 4],\n",
    "               conv_strides=[4, 3, 3, 2, 1],\n",
    "               pool_strides=[5, 5, 4, 4, 3]\n",
    "               ).to(config.device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), config.learning_rate)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "Utils.train(train_loader, val_loader, model, opt, lossfunc=crit, config=config, show_batch_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62009dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test model\n",
    "Utils.test(test_loader, model, lossfunc=crit, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e1d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "torch.save(model.state_dict(), \"../model_fma.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from src.FMA import load_fma_small_testset\n",
    "from src.FMA import FMAAudioDataset\n",
    "\n",
    "\n",
    "corrupted = [\"133297.wav\", \"098565.wav\", \"098567.wav\", \"098569.wav\", \"099134.wav\", \"108925.wav\"]  # optional\n",
    "# Load test set\n",
    "file_paths, labels = load_fma_small_testset(config, corrupted_files=corrupted)\n",
    "\n",
    "# Create Dataset\n",
    "dataset = FMAAudioDataset(\n",
    "    audio_files=file_paths,\n",
    "    labels=labels,\n",
    "    audio_path=config.fma_audio_dir_path,\n",
    "    sampling_rate=config.sampling_rate,\n",
    "    duration=augmentations.cropt_size\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Wrap in DataLoader\n",
    "fma_loader = DataLoader(dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "# Test the model\n",
    "Utils.test(fma_loader, model, lossfunc=crit, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef000fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in fma_loader:\n",
    "        data = data.to(config.device)\n",
    "        target = target.to(config.device)\n",
    "\n",
    "        logits = model(data)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_targets = np.array(all_targets)\n",
    "\n",
    "# Only keep hiphop(4), rock(9), pop(7)\n",
    "valid_classes = {4, 9, 7}\n",
    "\n",
    "mask = np.array([t in valid_classes for t in all_targets])\n",
    "\n",
    "filtered_targets = all_targets[mask]\n",
    "filtered_preds   = all_preds[mask]\n",
    "\n",
    "# Map 4→0 (hiphop), 9→1 (rock), 7→2 (pop)\n",
    "label_map = {4:0, 9:1, 7:2}\n",
    "inv_map   = {0:\"hiphop\", 1:\"rock\", 2:\"pop\"}\n",
    "\n",
    "y_true = np.array([label_map[x] for x in filtered_targets])\n",
    "\n",
    "# If the model predicts a genre outside 4/7/9, count it as \"unknown\"\n",
    "y_pred = np.array([label_map[p] if p in label_map else -1 for p in filtered_preds])\n",
    "\n",
    "# Remove -1 predictions before computing confusion matrix\n",
    "valid_pred_mask = y_pred != -1\n",
    "y_true = y_true[valid_pred_mask]\n",
    "y_pred = y_pred[valid_pred_mask]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\",\n",
    "            xticklabels=[inv_map[i] for i in range(3)],\n",
    "            yticklabels=[inv_map[i] for i in range(3)],\n",
    "            cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Hiphop / Rock / Pop)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77370bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(20+35+23)/(20+35+23+4+55+1+4+6+9)\n",
    "# 150 in our three genres, the rest in others (150 )\n",
    "\n",
    "# evaluate the classifier as: choose the highest probability out of the genres M,R,P and do not allow to to choose from the other seven genres. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "git",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
